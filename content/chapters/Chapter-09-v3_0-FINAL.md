# Chapter 9: AI + ì˜¨í†¨ë¡œì§€

**í•™ìŠµ ì‹œê°„:** 90ë¶„  
**ë‚œì´ë„:** â­â­â­â­â­  
**ì‹œë®¬ë ˆì´í„°:** 3D Knowledge Graph â­  
**ì‘ì„±ì¼:** 2025-11-09  
**ë²„ì „:** 3.0 FINAL

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ ì±•í„°ë¥¼ ë§ˆì¹˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. âœ… Graph RAGì˜ ê°œë…ê³¼ ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
2. âœ… LLM Hallucination ë¬¸ì œì™€ ì˜¨í†¨ë¡œì§€ í•´ê²°ì±…ì„ ì„¤ëª…í•œë‹¤
3. âœ… Vector DB + Knowledge Graph í†µí•© ë°©ë²•ì„ ìµíŒë‹¤
4. âœ… ì£¼ìš” AI ê¸°ì—…ì˜ ì˜¨í†¨ë¡œì§€ ì ‘ê·¼ë²•ì„ ë¹„êµí•œë‹¤
5. âœ… Neuro-Symbolic AI ê°œë…ì„ ì´í•´í•œë‹¤
6. âœ… Ontology-Augmented Generation (OAG)ì„ êµ¬í˜„í•œë‹¤
7. âœ… 2025-2030 AI + ì˜¨í†¨ë¡œì§€ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•œë‹¤

---

## ğŸ“š ëª©ì°¨

1. [AIì™€ ì˜¨í†¨ë¡œì§€ì˜ ë§Œë‚¨](#1-aiì™€-ì˜¨í†¨ë¡œì§€ì˜-ë§Œë‚¨)
2. [LLM Hallucination ë¬¸ì œ](#2-llm-hallucination-ë¬¸ì œ)
3. [Graph RAG ì™„ì „ ê°€ì´ë“œ](#3-graph-rag-ì™„ì „-ê°€ì´ë“œ)
4. [Vector DB + KG í†µí•©](#4-vector-db--kg-í†µí•©)
5. [ê¸°ì—…ë³„ ì ‘ê·¼ë²•](#5-ê¸°ì—…ë³„-ì ‘ê·¼ë²•)
6. [Ontology-Augmented Generation](#6-ontology-augmented-generation)
7. [Neuro-Symbolic AI](#7-neuro-symbolic-ai)
8. [ì‹¤ì œ êµ¬í˜„ ì‚¬ë¡€](#8-ì‹¤ì œ-êµ¬í˜„-ì‚¬ë¡€)
9. [ì‹¤ìŠµ: RAG ì‹œìŠ¤í…œ](#9-ì‹¤ìŠµ-rag-ì‹œìŠ¤í…œ)
10. [Graph Neural Networks](#10-graph-neural-networks)
11. [ë¯¸ë˜ íŠ¸ë Œë“œ](#11-ë¯¸ë˜-íŠ¸ë Œë“œ)
12. [ìš”ì•½ê³¼ ë‹¤ìŒ ë‹¨ê³„](#12-ìš”ì•½ê³¼-ë‹¤ìŒ-ë‹¨ê³„)

---

## 1. AIì™€ ì˜¨í†¨ë¡œì§€ì˜ ë§Œë‚¨

### ì™œ AIì— ì˜¨í†¨ë¡œì§€ê°€ í•„ìš”í•œê°€?

**LLMì˜ í•œê³„:**
```
ChatGPT, Claude, Gemini ë“±:
- í™˜ê°(Hallucination): ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš© ìƒì„±
- ì§€ì‹ ê³ ì •: í•™ìŠµ ì‹œì  ì´í›„ ì •ë³´ ëª¨ë¦„
- ì¶”ë¡  ë¶€ì¡±: ë…¼ë¦¬ì  ì¶”ë¡  ì•½í•¨
- ì„¤ëª… ë¶ˆê°€: "ì™œ?"ì— ëŒ€í•œ ë‹µ ì—†ìŒ
```

**ì˜¨í†¨ë¡œì§€ì˜ ê°•ì :**
```
- ì‚¬ì‹¤ ê²€ì¦: ëª…í™•í•œ ì§€ì‹ ê·¸ë˜í”„
- ìµœì‹  ì •ë³´: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
- ë…¼ë¦¬ ì¶”ë¡ : ì¶”ë¡  ì—”ì§„ìœ¼ë¡œ ìƒˆ ì§€ì‹ ë„ì¶œ
- ì„¤ëª… ê°€ëŠ¥: ê´€ê³„ ì¶”ì ìœ¼ë¡œ ê·¼ê±° ì œì‹œ
```

### í†µí•©ì˜ ì´ì 

**AI + ì˜¨í†¨ë¡œì§€ = Neuro-Symbolic AI**

```
LLM (ì‹ ê²½ë§)           â† ìì—°ì–´ ì´í•´, ìƒì„±
     â†•
ì˜¨í†¨ë¡œì§€ (ê¸°í˜¸)        â† ì§€ì‹ êµ¬ì¡°, ì¶”ë¡ 

â†’ ê°ìì˜ ê°•ì  í™œìš©
â†’ ì„œë¡œì˜ ì•½ì  ë³´ì™„
```

### ROI ë°ì´í„°

| ê¸°ì—… | í”„ë¡œì íŠ¸ | ì •í™•ë„ ê°œì„  | í™˜ê° ê°ì†Œ |
|------|---------|-----------|----------|
| **Microsoft** | Bing Chat + KG | +40% | -60% |
| **Google** | Gemini + KG | +35% | -55% |
| **IBM** | Watson + Ontology | +50% | -70% |
| **Anthropic** | Claude + Citations | +30% | -50% |

---

## 2. LLM Hallucination ë¬¸ì œ

### Hallucinationì´ë€?

**Hallucination (í™˜ê°)**ì€ LLMì´ ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.

**ì˜ˆì‹œ 1: ì˜ëª»ëœ ì‚¬ì‹¤**
```
Q: "Elon Muskê°€ ì°½ì—…í•œ íšŒì‚¬ëŠ”?"
A (í™˜ê°): "Tesla, SpaceX, OpenAI, Neuralink, The Boring Company"

ë¬¸ì œ: OpenAIëŠ” Sam Altmanì´ CEOì´ê³ , Elon MuskëŠ” ì´ˆê¸° íˆ¬ììì¼ ë¿
```

**ì˜ˆì‹œ 2: ì—†ëŠ” ì¸ìš©**
```
Q: "AI ìœ¤ë¦¬ì— ëŒ€í•œ ë…¼ë¬¸ ì¶”ì²œí•´ì¤˜"
A (í™˜ê°): "Smith et al. (2023) 'Ethics in AI' - Nature ê²Œì¬"

ë¬¸ì œ: ì´ëŸ° ë…¼ë¬¸ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
```

**ì˜ˆì‹œ 3: ì˜ëª»ëœ ë‚ ì§œ**
```
Q: "2024ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ìˆ˜ìƒìëŠ”?"
A (í™˜ê°): "John Doe (ì–‘ì ì»´í“¨íŒ… ì—°êµ¬)"

ë¬¸ì œ: 2024ë…„ ìˆ˜ìƒìëŠ” ë‹¤ë¥¸ ì‚¬ëŒ
```

### í™˜ê°ì˜ ì›ì¸

**1. í•™ìŠµ ë°ì´í„°ì˜ í•œê³„**
```
- ì˜ëª»ëœ ì •ë³´ í¬í•¨
- í¸í–¥ëœ ë°ì´í„°
- ì˜¤ë˜ëœ ì •ë³´
```

**2. í™•ë¥ ì  ìƒì„±**
```
LLMì€ "ë‹¤ìŒ ë‹¨ì–´ í™•ë¥ "ë¡œ ë™ì‘
â†’ ê°€ì¥ ê·¸ëŸ´ë“¯í•œ ë§ì„ ìƒì„±
â†’ ì‚¬ì‹¤ ì—¬ë¶€ì™€ ë¬´ê´€
```

**3. ì§€ì‹ ê³ ì •**
```
GPT-4: 2023ë…„ 4ì›”ê¹Œì§€
Claude: 2025ë…„ 1ì›”ê¹Œì§€

â†’ ê·¸ ì´í›„ ì •ë³´ ëª¨ë¦„
â†’ ì¶”ì¸¡ìœ¼ë¡œ ë‹µë³€
```

### ì˜¨í†¨ë¡œì§€ í•´ê²°ì±…

**1. ì‚¬ì‹¤ ê²€ì¦**
```turtle
# LLM ë‹µë³€
"Elon Musk founded OpenAI"

# ì˜¨í†¨ë¡œì§€ í™•ì¸
:Elon_Musk :founded :Tesla , :SpaceX .
:Sam_Altman :founded :OpenAI .
:Elon_Musk :invested_in :OpenAI .

# ê²°ê³¼: í™˜ê° íƒì§€ âœ…
```

**2. ì§€ì‹ ê·¸ë¼ìš´ë”©**
```python
def answer_with_kg(question):
    # 1. LLMì´ ì´ˆì•ˆ ìƒì„±
    draft = llm.generate(question)
    
    # 2. ì˜¨í†¨ë¡œì§€ì—ì„œ ì‚¬ì‹¤ í™•ì¸
    facts = kg.query(extract_entities(draft))
    
    # 3. ì‚¬ì‹¤ê³¼ ë¹„êµ
    verified_draft = verify_facts(draft, facts)
    
    # 4. í‹€ë¦° ë¶€ë¶„ ìˆ˜ì •
    if not verified_draft.is_valid:
        corrected = llm.regenerate(question, facts)
        return corrected
    
    return draft
```

**3. ì¶œì²˜ ì œê³µ**
```python
answer = """
Elon MuskëŠ” Teslaì™€ SpaceXì˜ ì°½ì—…ìì…ë‹ˆë‹¤.
OpenAIëŠ” Sam Altmanì´ ì°½ì—…í–ˆìœ¼ë©°, Elon MuskëŠ” ì´ˆê¸° íˆ¬ììì…ë‹ˆë‹¤.

ì¶œì²˜:
- Tesla ì°½ì—…: <http://kg.example.org/Tesla>
- SpaceX ì°½ì—…: <http://kg.example.org/SpaceX>
- OpenAI ê´€ê³„: <http://kg.example.org/OpenAI>
"""
```

---

## 3. Graph RAG ì™„ì „ ê°€ì´ë“œ

### RAGë€?

**RAG (Retrieval-Augmented Generation)**ëŠ” ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ê¸°ë²•ì…ë‹ˆë‹¤.

**ì‘ë™ ì›ë¦¬:**
```
1. ì§ˆë¬¸ ë°›ê¸°
2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
3. ë¬¸ì„œ + ì§ˆë¬¸ â†’ LLM
4. LLMì´ ë‹µë³€ ìƒì„±
```

**ì¼ë°˜ RAG vs Graph RAG:**

| êµ¬ë¶„ | ì¼ë°˜ RAG | Graph RAG |
|------|----------|-----------|
| ê²€ìƒ‰ ëŒ€ìƒ | í…ìŠ¤íŠ¸ ë¬¸ì„œ | ì§€ì‹ ê·¸ë˜í”„ |
| ê²€ìƒ‰ ë°©ë²• | Vector Similarity | Graph Traversal |
| ê´€ê³„ í™œìš© | âŒ | âœ… |
| ì¶”ë¡  | âŒ | âœ… |
| ì„¤ëª…ë ¥ | ë‚®ìŒ | ë†’ìŒ |

### Graph RAG ì•„í‚¤í…ì²˜

```
[ì§ˆë¬¸: "Elon Muskê°€ ì°½ì—…í•œ AI íšŒì‚¬ëŠ”?"]
       â†“
[1. ì—”í‹°í‹° ì¶”ì¶œ]
   Entities: [Elon Musk, AI íšŒì‚¬]
       â†“
[2. ê·¸ë˜í”„ ê²€ìƒ‰]
   SPARQL:
   SELECT ?company
   WHERE {
       :Elon_Musk :founded ?company .
       ?company :industry "AI" .
   }
       â†“
[3. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ]
   Elon Musk â†’ founded â†’ [Neuralink]
   Neuralink â†’ industry â†’ AI
   Neuralink â†’ focus â†’ Brain-Computer Interface
       â†“
[4. LLM ìƒì„±]
   Context: [ì„œë¸Œê·¸ë˜í”„ ì •ë³´]
   Question: [ì›ë˜ ì§ˆë¬¸]
   â†’ Answer: "Neuralink (ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤)"
       â†“
[5. ì¶œì²˜ ì œê³µ]
   Citations: [ê·¸ë˜í”„ ë…¸ë“œ URI]
```

### Python êµ¬í˜„

```python
from langchain import LLM
from rdflib import Graph
from SPARQLWrapper import SPARQLWrapper

class GraphRAG:
    def __init__(self, kg_endpoint, llm):
        self.kg = SPARQLWrapper(kg_endpoint)
        self.llm = llm
    
    def answer(self, question):
        # 1. ì—”í‹°í‹° ì¶”ì¶œ
        entities = self.extract_entities(question)
        
        # 2. ê·¸ë˜í”„ ê²€ìƒ‰
        subgraph = self.query_kg(entities)
        
        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.format_subgraph(subgraph)
        
        # 4. LLM ìƒì„±
        prompt = f"""
        Question: {question}
        
        Knowledge Graph Context:
        {context}
        
        Answer the question using ONLY the information from the Knowledge Graph.
        Cite your sources using URIs.
        """
        
        answer = self.llm.generate(prompt)
        
        return answer
    
    def extract_entities(self, question):
        """ì§ˆë¬¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (NER)"""
        # LLM ë˜ëŠ” NER ëª¨ë¸ ì‚¬ìš©
        prompt = f"Extract entities from: {question}"
        entities = self.llm.generate(prompt)
        return entities
    
    def query_kg(self, entities):
        """ì—”í‹°í‹° ê´€ë ¨ ì„œë¸Œê·¸ë˜í”„ ê²€ìƒ‰"""
        query = f"""
        SELECT ?s ?p ?o
        WHERE {{
            VALUES ?entity {{ {' '.join(entities)} }}
            ?entity ?p1 ?o1 .
            ?o1 ?p ?o .
        }}
        LIMIT 100
        """
        
        self.kg.setQuery(query)
        results = self.kg.query().convert()
        return results
    
    def format_subgraph(self, subgraph):
        """ì„œë¸Œê·¸ë˜í”„ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        formatted = []
        for result in subgraph:
            triple = f"{result['s']} {result['p']} {result['o']}"
            formatted.append(triple)
        return "\n".join(formatted)

# ì‚¬ìš© ì˜ˆì œ
rag = GraphRAG(
    kg_endpoint="http://localhost:7200/repositories/kb",
    llm=ChatGPT()
)

answer = rag.answer("Elon Muskê°€ ì°½ì—…í•œ AI íšŒì‚¬ëŠ”?")
print(answer)
```

### ê³ ê¸‰ ê¸°ë²•

**1. Multi-Hop Reasoning**
```sparql
# 2-hop ê²€ìƒ‰: "ì¹œêµ¬ì˜ ì¹œêµ¬"
SELECT ?person2
WHERE {
    :User_A :friend ?person1 .
    ?person1 :friend ?person2 .
    FILTER(?person2 != :User_A)
}
```

**2. Path Finding**
```python
def find_path(start_entity, end_entity, max_hops=3):
    """ë‘ ì—”í‹°í‹° ê°„ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸°"""
    query = f"""
    SELECT ?path
    WHERE {{
        {start_entity} (^?p1/!?p2){{1,{max_hops}}} {end_entity} .
    }}
    """
    return execute_query(query)
```

**3. Contextual Expansion**
```python
def expand_context(entity, radius=2):
    """ì—”í‹°í‹° ì£¼ë³€ N-hop í™•ì¥"""
    query = f"""
    SELECT ?s ?p ?o
    WHERE {{
        {entity} ((!?p1/!?p2){{0,{radius}}}) ?s .
        ?s ?p ?o .
    }}
    """
    return execute_query(query)
```

---

## 4. Vector DB + KG í†µí•©

### í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜

**Vector DB + Knowledge Graph = ìµœìƒì˜ ì¡°í•©**

```
Vector DB (ì„ë² ë”©)       Knowledge Graph (êµ¬ì¡°)
    â†“                         â†“
[ì˜ë¯¸ ìœ ì‚¬ë„ ê²€ìƒ‰]        [ì •í™•í•œ ê´€ê³„ ì¶”ë¡ ]
    â†“                         â†“
         [í†µí•© ê²€ìƒ‰]
              â†“
          [LLM ìƒì„±]
```

### ì£¼ìš” ì†”ë£¨ì…˜

**1. Neo4j + Vector Search**
```python
from neo4j import GraphDatabase

# Neo4jì— ì„ë² ë”© ì €ì¥
CREATE (n:Document {
    title: "AI Ethics",
    embedding: [0.1, 0.2, ..., 0.768]
})

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
query = """
CALL db.index.vector.queryNodes(
    'document_embeddings',
    5,
    $embedding
) YIELD node, score
MATCH (node)-[:RELATED_TO]->(entity)
RETURN node, entity, score
"""
```

**2. Weaviate (Native)**
```python
import weaviate

client = weaviate.Client("http://localhost:8080")

# ìŠ¤í‚¤ë§ˆ ì •ì˜ (ì˜¨í†¨ë¡œì§€ì™€ ìœ ì‚¬)
schema = {
    "class": "Document",
    "vectorizer": "text2vec-openai",
    "properties": [
        {"name": "title", "dataType": ["string"]},
        {"name": "content", "dataType": ["text"]},
        {"name": "relatedTo", "dataType": ["Document"]}
    ]
}

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
result = client.query.get(
    "Document",
    ["title", "content"]
).with_hybrid(
    query="AI ethics",
    alpha=0.5  # 0.5 = Vector + Keyword
).with_additional(
    ["score"]
).do()
```

**3. Pinecone + RDF**
```python
import pinecone
from rdflib import Graph

# Pineconeì— RDF ë©”íƒ€ë°ì´í„° ì €ì¥
pinecone.init(api_key="...")
index = pinecone.Index("kb")

# ë²¡í„° + RDF ë©”íƒ€ë°ì´í„°
index.upsert([
    {
        "id": "doc1",
        "values": embedding,  # 768ì°¨ì› ë²¡í„°
        "metadata": {
            "rdf:type": "schema:Article",
            "schema:author": "John Doe",
            "dc:subject": "AI Ethics"
        }
    }
])

# ê²€ìƒ‰
results = index.query(
    vector=query_embedding,
    top_k=5,
    filter={
        "rdf:type": "schema:Article",
        "dc:subject": "AI Ethics"
    }
)
```

### í†µí•© ì „ëµ

**ì „ëµ 1: Vector-First**
```python
def vector_first_search(query):
    # 1. Vector DBë¡œ ë¬¸ì„œ ê²€ìƒ‰
    docs = vector_db.search(query, top_k=10)
    
    # 2. ë¬¸ì„œ â†’ ì—”í‹°í‹° ë§¤í•‘
    entities = extract_entities(docs)
    
    # 3. Knowledge Graphë¡œ ê´€ê³„ í™•ì¥
    subgraph = kg.expand(entities)
    
    # 4. LLM ìƒì„±
    answer = llm.generate(query, context=subgraph)
    return answer
```

**ì „ëµ 2: Graph-First**
```python
def graph_first_search(query):
    # 1. Knowledge Graphë¡œ ì—”í‹°í‹° ê²€ìƒ‰
    entities = kg.query(query)
    
    # 2. ì—”í‹°í‹° â†’ ë¬¸ì„œ ë§¤í•‘
    doc_ids = get_doc_ids(entities)
    
    # 3. Vector DBë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = vector_db.get(doc_ids)
    
    # 4. LLM ìƒì„±
    answer = llm.generate(query, context=docs)
    return answer
```

**ì „ëµ 3: Parallel (ê°€ì¥ íš¨ê³¼ì )**
```python
def parallel_search(query):
    # 1. ë³‘ë ¬ ê²€ìƒ‰
    vector_results = vector_db.search(query, top_k=5)
    graph_results = kg.query(query, limit=5)
    
    # 2. ê²°ê³¼ ë³‘í•© ë° ìˆœìœ„ ì¬ì¡°ì •
    combined = rerank(vector_results, graph_results)
    
    # 3. LLM ìƒì„±
    answer = llm.generate(query, context=combined)
    return answer
```

---

## 5. ê¸°ì—…ë³„ ì ‘ê·¼ë²•

### 1. OpenAI + Knowledge Graph

**ì ‘ê·¼ë²•:** ì™¸ë¶€ KGë¥¼ Function Callingìœ¼ë¡œ í†µí•©

```python
import openai

# Function ì •ì˜
functions = [
    {
        "name": "query_knowledge_graph",
        "description": "Query a knowledge graph for factual information",
        "parameters": {
            "type": "object",
            "properties": {
                "entity": {"type": "string"},
                "relation": {"type": "string"}
            }
        }
    }
]

# GPT-4 í˜¸ì¶œ
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Who founded Tesla?"}
    ],
    functions=functions
)

# Function Call ì²˜ë¦¬
if response.choices[0].message.get("function_call"):
    function_name = response.choices[0].message["function_call"]["name"]
    arguments = response.choices[0].message["function_call"]["arguments"]
    
    # KG ì¿¼ë¦¬
    result = query_knowledge_graph(**arguments)
    
    # ê²°ê³¼ë¥¼ ë‹¤ì‹œ GPT-4ì— ì „ë‹¬
    final_response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "user", "content": "Who founded Tesla?"},
            {"role": "function", "name": function_name, "content": str(result)}
        ]
    )
```

### 2. Google (Gemini + Knowledge Graph)

**ì ‘ê·¼ë²•:** ë‚´ë¶€ Knowledge Graph ë„¤ì´í‹°ë¸Œ í†µí•©

```python
import google.generativeai as genai

# Gemini + KG
model = genai.GenerativeModel(
    model_name="gemini-pro",
    generation_config={
        "grounding": {
            "knowledge_graph": {
                "endpoint": "https://kg.googleapis.com",
                "enabled": True
            }
        }
    }
)

# ì¿¼ë¦¬
response = model.generate_content("Who founded Tesla?")

# ìë™ìœ¼ë¡œ KG ì°¸ì¡°
print(response.text)  # "Elon Musk founded Tesla in 2003"
print(response.grounding_metadata)  # KG ì¶œì²˜ ì •ë³´
```

### 3. Anthropic (Claude + Citations)

**ì ‘ê·¼ë²•:** ì™¸ë¶€ ë„êµ¬ í†µí•© + ì¶œì²˜ ëª…ì‹œ

```python
import anthropic

# Claude with Tools
client = anthropic.Anthropic(api_key="...")

response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1024,
    tools=[
        {
            "name": "knowledge_graph_query",
            "description": "Query a knowledge graph",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                }
            }
        }
    ],
    messages=[
        {"role": "user", "content": "Who founded Tesla?"}
    ]
)

# Tool ì‚¬ìš© ì²˜ë¦¬
if response.stop_reason == "tool_use":
    tool_use = response.content[0]
    
    # KG ì¿¼ë¦¬
    kg_result = query_kg(tool_use.input["query"])
    
    # ê²°ê³¼ë¥¼ Claudeì— ì „ë‹¬
    final_response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1024,
        messages=[
            {"role": "user", "content": "Who founded Tesla?"},
            {"role": "assistant", "content": response.content},
            {"role": "user", "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use.id,
                "content": kg_result
            }]}
        ]
    )
```

### 4. Meta (LLaMA + Graph)

**ì ‘ê·¼ë²•:** ì˜¤í”ˆì†ŒìŠ¤ + Graph RAG

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# LLaMA ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-70b-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-70b-hf")

# Graph RAG
def llama_graph_rag(question):
    # 1. KG ê²€ìƒ‰
    kg_context = query_kg(question)
    
    # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    Question: {question}
    
    Knowledge Graph:
    {kg_context}
    
    Answer based on the Knowledge Graph:
    """
    
    # 3. LLaMA ìƒì„±
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=512)
    answer = tokenizer.decode(outputs[0])
    
    return answer
```

### ë¹„êµí‘œ

| ê¸°ì—… | ëª¨ë¸ | KG í†µí•© | ì¥ì  | ë‹¨ì  |
|------|------|---------|------|------|
| OpenAI | GPT-4 | Function Calling | ìœ ì—°ì„± ë†’ìŒ | ìˆ˜ë™ êµ¬í˜„ í•„ìš” |
| Google | Gemini | Native | ìë™í™”ë¨ | íì‡„ì  |
| Anthropic | Claude | Tool Use | ì¶œì²˜ ëª…í™• | ì™¸ë¶€ KG í•„ìš” |
| Meta | LLaMA | ì™¸ë¶€ í†µí•© | ì˜¤í”ˆì†ŒìŠ¤ | ì„±ëŠ¥ ë‚®ìŒ |

---

## 6. Ontology-Augmented Generation

### OAGë€?

**Ontology-Augmented Generation (OAG)**ëŠ” ì˜¨í†¨ë¡œì§€ë¡œ LLM ìƒì„±ì„ ê°•í™”í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

**RAG vs OAG:**

| êµ¬ë¶„ | RAG | OAG |
|------|-----|-----|
| ê²€ìƒ‰ ëŒ€ìƒ | í…ìŠ¤íŠ¸ ë¬¸ì„œ | ì˜¨í†¨ë¡œì§€ |
| êµ¬ì¡° | ë¹„êµ¬ì¡°í™” | êµ¬ì¡°í™” |
| ì¶”ë¡  | âŒ | âœ… (ì¶”ë¡  ì—”ì§„) |
| ì¼ê´€ì„± | ë‚®ìŒ | ë†’ìŒ |
| ì„¤ëª…ë ¥ | ë‚®ìŒ | ë†’ìŒ |

### OAG ì•„í‚¤í…ì²˜

```
[ì§ˆë¬¸]
  â†“
[ì—”í‹°í‹° ì¶”ì¶œ]
  â†“
[ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬ + ì¶”ë¡ ]
  â†“
[ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ]
  â†“
[LLM ìƒì„± (ì˜¨í†¨ë¡œì§€ ê¸°ë°˜)]
  â†“
[ê²€ì¦ ë° ìˆ˜ì •]
  â†“
[ìµœì¢… ë‹µë³€ + ì¶œì²˜]
```

### Python êµ¬í˜„

```python
from owlready2 import get_ontology, sync_reasoner_pellet
from openai import ChatCompletion

class OntologyAugmentedGenerator:
    def __init__(self, ontology_path, llm):
        self.onto = get_ontology(ontology_path).load()
        self.llm = llm
    
    def answer(self, question):
        # 1. ì—”í‹°í‹° ì¶”ì¶œ
        entities = self.extract_entities(question)
        
        # 2. ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬
        facts = self.query_ontology(entities)
        
        # 3. ì¶”ë¡  ì‹¤í–‰
        inferred_facts = self.reason(facts)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.format_facts(facts + inferred_facts)
        
        # 5. LLM ìƒì„±
        prompt = f"""
        Question: {question}
        
        Ontology Knowledge:
        {context}
        
        Rules:
        - Answer ONLY using the ontology knowledge
        - Cite your sources using URIs
        - If information is not in the ontology, say "I don't know"
        
        Answer:
        """
        
        answer = self.llm.generate(prompt)
        
        # 6. ê²€ì¦
        validated_answer = self.validate(answer, facts)
        
        return validated_answer
    
    def query_ontology(self, entities):
        """ì—”í‹°í‹° ê´€ë ¨ ì‚¬ì‹¤ ì¿¼ë¦¬"""
        facts = []
        for entity_name in entities:
            entity = self.onto.search_one(iri=f"*{entity_name}")
            if entity:
                # ì†ì„± ì¶”ì¶œ
                for prop in entity.get_properties():
                    for value in prop[entity]:
                        facts.append((entity, prop, value))
        return facts
    
    def reason(self, facts):
        """ì¶”ë¡  ì—”ì§„ ì‹¤í–‰"""
        with self.onto:
            sync_reasoner_pellet()
        
        # ì¶”ë¡ ëœ ìƒˆë¡œìš´ ì‚¬ì‹¤ ì¶”ì¶œ
        inferred = []
        for individual in self.onto.individuals():
            for prop in individual.get_properties():
                for value in prop[individual]:
                    if (individual, prop, value) not in facts:
                        inferred.append((individual, prop, value))
        
        return inferred
    
    def validate(self, answer, facts):
        """ë‹µë³€ ê²€ì¦"""
        # ë‹µë³€ì—ì„œ ì£¼ì¥ ì¶”ì¶œ
        claims = self.extract_claims(answer)
        
        # ê° ì£¼ì¥ì´ factsì— ìˆëŠ”ì§€ í™•ì¸
        for claim in claims:
            if not self.verify_claim(claim, facts):
                # ê²€ì¦ ì‹¤íŒ¨ â†’ ìˆ˜ì • ìš”ì²­
                return self.regenerate_with_correction(claim)
        
        return answer

# ì‚¬ìš© ì˜ˆì œ
oag = OntologyAugmentedGenerator(
    ontology_path="/path/to/knowledge.owl",
    llm=ChatGPT()
)

answer = oag.answer("What companies did Elon Musk found?")
print(answer)
```

### ì‹¤ì œ ì˜ˆì œ

**ì˜¨í†¨ë¡œì§€:**
```turtle
@prefix : <http://example.org/> .

:Elon_Musk a :Person ;
    :founded :Tesla , :SpaceX , :Neuralink , :The_Boring_Company .

:Tesla a :Company ;
    :industry "Automotive" ;
    :founded_year 2003 .

:SpaceX a :Company ;
    :industry "Aerospace" ;
    :founded_year 2002 .
```

**OAG í”„ë¡œì„¸ìŠ¤:**
```python
# 1. ì§ˆë¬¸
question = "What companies did Elon Musk found?"

# 2. ì—”í‹°í‹° ì¶”ì¶œ
entities = ["Elon Musk", "companies"]

# 3. ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬
facts = [
    (:Elon_Musk, :founded, :Tesla),
    (:Elon_Musk, :founded, :SpaceX),
    (:Elon_Musk, :founded, :Neuralink),
    (:Elon_Musk, :founded, :The_Boring_Company),
    (:Tesla, :industry, "Automotive"),
    (:Tesla, :founded_year, 2003),
    ...
]

# 4. ì¶”ë¡  (ì—†ìŒ - ëª…ì‹œì  ì‚¬ì‹¤ë§Œ)

# 5. LLM ìƒì„±
answer = """
Elon Musk founded the following companies:
1. Tesla (Automotive, 2003)
2. SpaceX (Aerospace, 2002)
3. Neuralink
4. The Boring Company

Sources:
- http://example.org/Elon_Musk
- http://example.org/Tesla
- http://example.org/SpaceX
"""

# 6. ê²€ì¦ âœ… (ëª¨ë‘ ì˜¨í†¨ë¡œì§€ì— ì¡´ì¬)
```

---

## 7. Neuro-Symbolic AI

### ê°œë…

**Neuro-Symbolic AI**ëŠ” ì‹ ê²½ë§(Neural)ê³¼ ê¸°í˜¸ ì¶”ë¡ (Symbolic)ì„ ê²°í•©í•œ AIì…ë‹ˆë‹¤.

**êµ¬ì„± ìš”ì†Œ:**
```
Neural (ì‹ ê²½ë§)          Symbolic (ê¸°í˜¸)
- íŒ¨í„´ ì¸ì‹              - ë…¼ë¦¬ ì¶”ë¡ 
- ìì—°ì–´ ì²˜ë¦¬            - ê·œì¹™ ê¸°ë°˜
- ì´ë¯¸ì§€ ì¸ì‹            - ì§€ì‹ ê·¸ë˜í”„
- í†µê³„ì  í•™ìŠµ            - ì˜¨í†¨ë¡œì§€
```

### ì•„í‚¤í…ì²˜

```
Input (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸)
    â†“
[Neural Network]
    - íŠ¹ì§• ì¶”ì¶œ
    - íŒ¨í„´ ì¸ì‹
    â†“
[ì¤‘ê°„ í‘œí˜„]
    - ì‹¬ë³¼ë¡œ ë³€í™˜
    â†“
[Symbolic Reasoner]
    - ë…¼ë¦¬ ì¶”ë¡ 
    - ê·œì¹™ ì ìš©
    â†“
Output (ê²°ê³¼ + ì„¤ëª…)
```

### ì˜ˆì œ: ì˜ë£Œ ì§„ë‹¨

**Neural ë¶€ë¶„:**
```python
import torch
import torch.nn as nn

class MedicalImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, 3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            ...
        )
        self.fc = nn.Linear(512, 100)  # 100 ì§ˆë³‘ ë¶„ë¥˜
    
    def forward(self, x):
        features = self.cnn(x)
        logits = self.fc(features)
        return logits

# ì˜ˆì¸¡
model = MedicalImageClassifier()
prediction = model(xray_image)
disease_id = torch.argmax(prediction)
```

**Symbolic ë¶€ë¶„:**
```turtle
@prefix med: <http://medical.example.org/> .

# ì˜¨í†¨ë¡œì§€
med:Pneumonia a med:Disease ;
    med:symptom med:Cough , med:Fever , med:Difficulty_Breathing ;
    med:treatment med:Antibiotics ;
    med:risk_factor med:Smoking , med:Old_Age .

# ì¶”ë¡  ê·œì¹™
IF patient has Cough
   AND patient has Fever
   AND patient has Difficulty_Breathing
   AND xray shows Lung_Opacity
THEN patient likely has Pneumonia
```

**í†µí•©:**
```python
def neuro_symbolic_diagnosis(patient, xray_image):
    # 1. Neural: ì—‘ìŠ¤ë ˆì´ ë¶„ì„
    cnn_prediction = cnn_model(xray_image)
    suspected_disease = get_disease_name(cnn_prediction)
    
    # 2. Symbolic: ì¦ìƒ ë§¤ì¹­
    patient_symptoms = patient.get_symptoms()
    ontology_symptoms = kg.get_symptoms(suspected_disease)
    
    # 3. ì¶”ë¡ 
    if match_rate(patient_symptoms, ontology_symptoms) > 0.7:
        # 4. ì¹˜ë£Œ ì¶”ì²œ
        treatment = kg.get_treatment(suspected_disease)
        
        # 5. ì„¤ëª… ìƒì„±
        explanation = f"""
        ì§„ë‹¨: {suspected_disease}
        
        ê·¼ê±°:
        - CNN ì˜ˆì¸¡: {cnn_prediction.confidence}
        - ì¦ìƒ ë§¤ì¹­: {match_rate}
        - ì—‘ìŠ¤ë ˆì´ ì†Œê²¬: {xray_findings}
        
        ì˜¨í†¨ë¡œì§€ ì§€ì‹:
        - ì¦ìƒ: {ontology_symptoms}
        - ì¹˜ë£Œ: {treatment}
        - ìœ„í—˜ ìš”ì¸: {risk_factors}
        """
        
        return {
            "diagnosis": suspected_disease,
            "treatment": treatment,
            "explanation": explanation
        }
```

### ì´ì 

**1. ì„¤ëª… ê°€ëŠ¥ì„±**
```
ì¼ë°˜ DL: "ì´ ì´ë¯¸ì§€ëŠ” íë ´ì…ë‹ˆë‹¤" (ë¸”ë™ë°•ìŠ¤)

Neuro-Symbolic: "ì´ ì´ë¯¸ì§€ëŠ” íë ´ì…ë‹ˆë‹¤.
- CNNì´ í ë¶ˆíˆ¬ëª…ë„ ê°ì§€ (95% ì‹ ë¢°ë„)
- í™˜ì ì¦ìƒ (ê¸°ì¹¨, ë°œì—´)ì´ ì˜¨í†¨ë¡œì§€ì˜ íë ´ ì¦ìƒê³¼ ì¼ì¹˜
- ì¶”ë¡  ê·œì¹™ì— ë”°ë¼ íë ´ ì§„ë‹¨"
```

**2. ì‘ì€ ë°ì´í„°**
```
ì¼ë°˜ DL: ìˆ˜ë§Œ ì¥ ì´ë¯¸ì§€ í•„ìš”

Neuro-Symbolic: ì˜¨í†¨ë¡œì§€ ì§€ì‹ í™œìš©
â†’ ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
```

**3. ì¼ë°˜í™”**
```
ì¼ë°˜ DL: í•™ìŠµ ë°ì´í„° ë¶„í¬ì— í•œì •

Neuro-Symbolic: ë…¼ë¦¬ ì¶”ë¡ ìœ¼ë¡œ ì¼ë°˜í™”
â†’ ìƒˆë¡œìš´ ìƒí™©ì—ë„ ëŒ€ì‘
```

---

## 8. ì‹¤ì œ êµ¬í˜„ ì‚¬ë¡€

### 1. IBM Watson Health

**í”„ë¡œì íŠ¸:** ì•” ì§„ë‹¨ ë³´ì¡° ì‹œìŠ¤í…œ  
**ê¸°ìˆ :** Neuro-Symbolic AI + ì˜ë£Œ ì˜¨í†¨ë¡œì§€  
**ê²°ê³¼:** 96% ì •í™•ë„ (ì „ë¬¸ì˜ ìˆ˜ì¤€)

```python
class WatsonOncology:
    def __init__(self):
        self.dnn = load_image_classifier()
        self.kg = load_medical_ontology()
    
    def diagnose(self, patient_data):
        # 1. ì´ë¯¸ì§€ ë¶„ì„ (Neural)
        scan_result = self.dnn.predict(patient_data.scan)
        
        # 2. ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬ (Symbolic)
        cancer_type = scan_result.top_prediction
        treatment_options = self.kg.query(f"""
            SELECT ?treatment ?evidence
            WHERE {{
                :{cancer_type} med:treatment ?treatment .
                ?treatment med:evidence_level ?evidence .
            }}
            ORDER BY DESC(?evidence)
        """)
        
        # 3. ê°œì¸í™” ì¶”ì²œ
        personalized = self.filter_by_patient_profile(
            treatment_options,
            patient_data.age,
            patient_data.comorbidities
        )
        
        return {
            "diagnosis": cancer_type,
            "confidence": scan_result.confidence,
            "treatments": personalized,
            "explanation": self.generate_explanation()
        }
```

### 2. Google Health

**í”„ë¡œì íŠ¸:** ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ ìŠ¤í¬ë¦¬ë‹  
**ê¸°ìˆ :** Deep Learning + Knowledge Graph  
**ê²°ê³¼:** ì¸ê°„ ì „ë¬¸ì˜ë³´ë‹¤ ë†’ì€ ì •í™•ë„

```python
class DiabeticRetinopathyDetector:
    def __init__(self):
        self.cnn = load_retina_classifier()
        self.kg = load_ophthalmology_kg()
    
    def screen(self, retina_image, patient_history):
        # 1. CNN ë¶„ì„
        severity = self.cnn.predict(retina_image)
        
        # 2. KGë¡œ ìœ„í—˜ë„ í‰ê°€
        risk_factors = self.kg.get_risk_factors(patient_history)
        
        # 3. ì¢…í•© í‰ê°€
        if severity >= 3 and len(risk_factors) > 2:
            recommendation = "ì¦‰ì‹œ ì•ˆê³¼ ì „ë¬¸ì˜ ì§„ë£Œ"
        elif severity >= 2:
            recommendation = "6ê°œì›” ë‚´ ì¬ê²€ì‚¬"
        else:
            recommendation = "1ë…„ í›„ ì¬ê²€ì‚¬"
        
        return {
            "severity": severity,
            "risk_factors": risk_factors,
            "recommendation": recommendation
        }
```

### 3. Microsoft (Bing + KG)

**í”„ë¡œì íŠ¸:** Bing Chat (now Copilot)  
**ê¸°ìˆ :** GPT-4 + Bing Knowledge Graph  
**ê²°ê³¼:** í™˜ê° 60% ê°ì†Œ

```python
def bing_chat(query):
    # 1. Bing ê²€ìƒ‰ + KG
    search_results = bing_search(query)
    kg_facts = bing_kg.query(extract_entities(query))
    
    # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = combine(search_results, kg_facts)
    
    # 3. GPT-4 ìƒì„±
    prompt = f"""
    User Query: {query}
    
    Web Search Results:
    {search_results}
    
    Knowledge Graph:
    {kg_facts}
    
    Answer the query using BOTH web and KG.
    Cite your sources.
    """
    
    answer = gpt4.generate(prompt)
    
    # 4. ì‚¬ì‹¤ ê²€ì¦
    verified = verify_facts(answer, kg_facts)
    
    return verified
```

---

## 9. ì‹¤ìŠµ: RAG ì‹œìŠ¤í…œ

### ğŸ® 3D Knowledge Graph ì—´ê¸°

URL: https://kss.ai.kr/3d-graph

### ì‹¤ìŠµ ëª©í‘œ

**Graph RAG ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.

### Step 1: ì§€ì‹ ê·¸ë˜í”„ ì¤€ë¹„

```turtle
@prefix : <http://company.example.org/> .
@prefix schema: <http://schema.org/> .

# ì‚¬ëŒ
:Elon_Musk a schema:Person ;
    schema:name "Elon Musk" ;
    schema:birthDate "1971-06-28"^^xsd:date ;
    schema:nationality "USA" , "South Africa" ;
    schema:founder :Tesla , :SpaceX , :Neuralink .

:Sam_Altman a schema:Person ;
    schema:name "Sam Altman" ;
    schema:founder :OpenAI ;
    schema:role "CEO" .

# íšŒì‚¬
:Tesla a schema:Organization ;
    schema:name "Tesla" ;
    schema:industry "Automotive" ;
    schema:foundingDate "2003"^^xsd:gYear ;
    schema:ceo :Elon_Musk .

:SpaceX a schema:Organization ;
    schema:name "SpaceX" ;
    schema:industry "Aerospace" ;
    schema:foundingDate "2002"^^xsd:gYear ;
    schema:ceo :Elon_Musk .

:OpenAI a schema:Organization ;
    schema:name "OpenAI" ;
    schema:industry "AI" ;
    schema:foundingDate "2015"^^xsd:gYear ;
    schema:ceo :Sam_Altman .

# íˆ¬ì ê´€ê³„
:Elon_Musk :invested_in :OpenAI ;
              :investment_year "2015"^^xsd:gYear .
```

### Step 2: 3D Graph ì‹œê°í™”

1. "Import Data" í´ë¦­
2. ìœ„ Turtle ë°ì´í„° ë¶™ì—¬ë„£ê¸°
3. "Load Graph" í´ë¦­

**ê·¸ë˜í”„ êµ¬ì¡°:**
```
[Elon Musk] â”€founderâ”€> [Tesla]
            â”€founderâ”€> [SpaceX]
            â”€founderâ”€> [Neuralink]
            â”€invested_inâ”€> [OpenAI]

[Sam Altman] â”€founderâ”€> [OpenAI]
```

### Step 3: Graph RAG ì¿¼ë¦¬

**ì§ˆë¬¸ 1: "Elon Muskê°€ ì°½ì—…í•œ íšŒì‚¬ëŠ”?"**

```sparql
SELECT ?company ?industry ?year
WHERE {
    :Elon_Musk schema:founder ?company .
    ?company schema:industry ?industry ;
             schema:foundingDate ?year .
}
ORDER BY ?year
```

**ê²°ê³¼:**
```
SpaceX (Aerospace, 2002)
Tesla (Automotive, 2003)
Neuralink (Brain-Computer Interface, 2016)
```

**ì§ˆë¬¸ 2: "OpenAIì˜ ì°½ì—…ìëŠ” ëˆ„êµ¬?"**

```sparql
SELECT ?person ?role
WHERE {
    ?person schema:founder :OpenAI .
    OPTIONAL { ?person schema:role ?role }
}
```

**ê²°ê³¼:**
```
Sam Altman (CEO)
```

**ì§ˆë¬¸ 3: "Elon MuskëŠ” OpenAIì™€ ì–´ë–¤ ê´€ê³„?"**

```sparql
SELECT ?relation ?year
WHERE {
    :Elon_Musk ?relation :OpenAI .
    OPTIONAL { :Elon_Musk :investment_year ?year }
}
```

**ê²°ê³¼:**
```
invested_in (2015)
```

### Step 4: Python í†µí•©

```python
from rdflib import Graph
from openai import ChatCompletion

class SimpleGraphRAG:
    def __init__(self, kg_file):
        self.kg = Graph()
        self.kg.parse(kg_file, format="turtle")
    
    def answer(self, question):
        # 1. ì—”í‹°í‹° ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        entities = self.extract_entities(question)
        
        # 2. SPARQL ì¿¼ë¦¬ ìƒì„±
        query = self.generate_query(entities)
        
        # 3. KG ê²€ìƒ‰
        results = self.kg.query(query)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.format_results(results)
        
        # 5. GPT-4 ë‹µë³€ ìƒì„±
        prompt = f"""
        Question: {question}
        
        Knowledge Graph Information:
        {context}
        
        Answer the question based ONLY on the Knowledge Graph.
        """
        
        answer = ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return answer.choices[0].message.content
    
    def extract_entities(self, question):
        """ê°„ë‹¨í•œ ì—”í‹°í‹° ì¶”ì¶œ (ì •ê·œì‹ ê¸°ë°˜)"""
        import re
        # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ ì¶”ì¶œ
        entities = re.findall(r'\b[A-Z][a-z]+(?:\s[A-Z][a-z]+)*\b', question)
        return entities
    
    def generate_query(self, entities):
        """ì—”í‹°í‹° ê¸°ë°˜ SPARQL ì¿¼ë¦¬ ìƒì„±"""
        entity_str = ", ".join([f":{e.replace(' ', '_')}" for e in entities])
        query = f"""
        SELECT ?s ?p ?o
        WHERE {{
            VALUES ?entity {{ {entity_str} }}
            ?entity ?p ?o .
        }}
        """
        return query
    
    def format_results(self, results):
        """ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        lines = []
        for row in results:
            s, p, o = row
            lines.append(f"{s} {p} {o}")
        return "\n".join(lines)

# ì‚¬ìš©
rag = SimpleGraphRAG("knowledge.ttl")
answer = rag.answer("Who founded Tesla?")
print(answer)
```

---

## 10. Graph Neural Networks

### GNNì´ë€?

**Graph Neural Networks (GNN)**ì€ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ” ì‹ ê²½ë§ì…ë‹ˆë‹¤.

**íŠ¹ì§•:**
- ë…¸ë“œ ì„ë² ë”© í•™ìŠµ
- ê´€ê³„ íŒ¨í„´ ì¸ì‹
- ê·¸ë˜í”„ ì „ì²´ ì´í•´

### ì•„í‚¤í…ì²˜

```
ê·¸ë˜í”„ (Nodes + Edges)
    â†“
[Node Feature ì…ë ¥]
    â†“
[Graph Convolution Layer 1]
    - ì´ì›ƒ ë…¸ë“œ ì •ë³´ ì§‘ê³„
    â†“
[Graph Convolution Layer 2]
    - ë” ë„“ì€ ë²”ìœ„ ì§‘ê³„
    â†“
[Pooling]
    â†“
[Output Layer]
    - ë…¸ë“œ ë¶„ë¥˜ / ë§í¬ ì˜ˆì¸¡ / ê·¸ë˜í”„ ë¶„ë¥˜
```

### PyTorch Geometric ì˜ˆì œ

```python
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

# ê·¸ë˜í”„ ë°ì´í„°
edge_index = torch.tensor([
    [0, 1, 1, 2],  # source nodes
    [1, 0, 2, 1]   # target nodes
], dtype=torch.long)

x = torch.tensor([
    [1, 0],  # Node 0 features
    [0, 1],  # Node 1 features
    [1, 1]   # Node 2 features
], dtype=torch.float)

data = Data(x=x, edge_index=edge_index)

# GNN ëª¨ë¸
class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(2, 16)  # 2 features â†’ 16 hidden
        self.conv2 = GCNConv(16, 3)  # 16 hidden â†’ 3 classes
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        
        # Layer 1
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        
        # Layer 2
        x = self.conv2(x, edge_index)
        
        return F.log_softmax(x, dim=1)

# í•™ìŠµ
model = GCN()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()
for epoch in range(200):
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out, labels)
    loss.backward()
    optimizer.step()
```

### ì‘ìš©: ë§í¬ ì˜ˆì¸¡

```python
def predict_missing_links(graph):
    """ëˆ„ë½ëœ ë§í¬ ì˜ˆì¸¡"""
    # 1. GNNìœ¼ë¡œ ë…¸ë“œ ì„ë² ë”© í•™ìŠµ
    embeddings = gnn_model(graph)
    
    # 2. ëª¨ë“  ë…¸ë“œ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for i in range(len(embeddings)):
        for j in range(i+1, len(embeddings)):
            sim = cosine_similarity(embeddings[i], embeddings[j])
            similarities.append((i, j, sim))
    
    # 3. ìœ ì‚¬ë„ ë†’ì€ ìŒ = ëˆ„ë½ëœ ë§í¬ ê°€ëŠ¥ì„±
    sorted_pairs = sorted(similarities, key=lambda x: x[2], reverse=True)
    
    return sorted_pairs[:10]  # Top 10
```

---

## 11. ë¯¸ë˜ íŠ¸ë Œë“œ

### 2025-2030 ì˜ˆì¸¡

**1. Neuro-Symbolic AI ë³´í¸í™”**
```
2025: ì—°êµ¬ ë‹¨ê³„
2027: ìƒìš© ì œí’ˆ ì¶œì‹œ (IBM, Google)
2030: í‘œì¤€ AI ì•„í‚¤í…ì²˜
```

**2. Automated Ontology Learning**
```
AIê°€ ìë™ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ êµ¬ì¶•:
- í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ
- ì˜¨í†¨ë¡œì§€ ìë™ ìƒì„±
- ì¸ê°„ì€ ê²€í† ë§Œ
```

**3. Graph RAG í‘œì¤€í™”**
```
2025: Early Adopters
2027: ëŒ€ê¸°ì—… ë„ì…
2030: ëª¨ë“  LLM ì œí’ˆì— í†µí•©
```

**4. ì‹œì¥ ì„±ì¥**
```
Knowledge Graph ì‹œì¥:
2023: $1.2B
2028: $5.8B
2032: $9.23B

CAGR: 24.3%
```

### ê¸°ìˆ  ë¡œë“œë§µ

**2025:**
- Palantir + NVIDIA OAG ìƒìš©í™”
- OpenAI Function Calling ê³ ë„í™”
- Neo4j Vector Search ê°•í™”

**2026-2027:**
- ìë™ ì˜¨í†¨ë¡œì§€ í•™ìŠµ ë„êµ¬
- Neuro-Symbolic í”„ë ˆì„ì›Œí¬ í‘œì¤€í™”
- GNN + LLM í†µí•©

**2028-2030:**
- ì™„ì „ ìë™í™”ëœ ì§€ì‹ ê´€ë¦¬
- AIê°€ ì˜¨í†¨ë¡œì§€ ìë™ ìœ ì§€ë³´ìˆ˜
- ì¸ê°„ ìˆ˜ì¤€ ì¶”ë¡  AI

### ì§ì—… ì‹œì¥

**ìƒˆë¡œìš´ ì§ë¬´:**
1. **Ontology Engineer** ($120K-180K)
2. **Knowledge Graph Architect** ($150K-220K)
3. **Graph RAG Specialist** ($130K-190K)
4. **Neuro-Symbolic AI Researcher** ($160K-250K)

**í•„ìš” ìŠ¤í‚¬:**
- RDF, OWL, SPARQL
- Python, PyTorch
- Neo4j, GraphDB
- LLM APIs (OpenAI, Anthropic)
- Vector Databases (Pinecone, Weaviate)

---

## 12. ìš”ì•½ê³¼ ë‹¤ìŒ ë‹¨ê³„

### í•µì‹¬ ì •ë¦¬

**1. LLM + ì˜¨í†¨ë¡œì§€**
- í™˜ê° í•´ê²° (20-60% ê°œì„ )
- ì‚¬ì‹¤ ê²€ì¦
- ì¶”ë¡  ëŠ¥ë ¥ ê°•í™”
- ì„¤ëª… ê°€ëŠ¥ì„±

**2. Graph RAG**
- RAGì˜ ì§„í™”
- ê·¸ë˜í”„ ê²€ìƒ‰ + ì¶”ë¡ 
- ë” ì •í™•í•œ ë‹µë³€
- ì¶œì²˜ ëª…í™•

**3. ê¸°ì—…ë³„ ì ‘ê·¼**
- OpenAI: Function Calling
- Google: Native KG
- Anthropic: Tool Use
- Meta: Open Source

**4. Neuro-Symbolic AI**
- Neural + Symbolic ê²°í•©
- ì ì€ ë°ì´í„°ë¡œ í•™ìŠµ
- ì¼ë°˜í™” ëŠ¥ë ¥
- ì„¤ëª… ê°€ëŠ¥

**5. ë¯¸ë˜ íŠ¸ë Œë“œ**
- ìë™ ì˜¨í†¨ë¡œì§€ í•™ìŠµ
- Graph RAG í‘œì¤€í™”
- KG ì‹œì¥ $9.23B (2032)
- ìƒˆë¡œìš´ ì§ì—… ì°½ì¶œ

### ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

**Graph RAG í”„ë¡œì íŠ¸ ì‹œ:**
- [ ] LLM ì„ íƒ (GPT-4, Claude, Gemini)
- [ ] ì˜¨í†¨ë¡œì§€/KG êµ¬ì¶•
- [ ] Triple Store ì„¤ì • (GraphDB, Neo4j)
- [ ] Vector DB í†µí•© (ì„ íƒì‚¬í•­)
- [ ] RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- [ ] ê²€ì¦ ë° í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡œë•ì…˜ ë°°í¬

### ë‹¤ìŒ ì±•í„°

**Chapter 10: ë¯¸ë˜ íŠ¸ë Œë“œ (ìµœì¢… ì±•í„°)**

ì˜¨í†¨ë¡œì§€ì˜ ë¯¸ë˜ì™€ ì—¬ëŸ¬ë¶„ì˜ ì»¤ë¦¬ì–´!
- ì‚°ì—…ë³„ í™œìš© ì „ë§
- ê¸°ìˆ  ë¡œë“œë§µ 2025-2030
- ì»¤ë¦¬ì–´ íŒ¨ìŠ¤ ê°€ì´ë“œ
- í•™ìŠµ ë¦¬ì†ŒìŠ¤
- ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬
- **í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ 10ê°œ!**

---

## ğŸ“ ì—°ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: RAG vs Graph RAG

ì¼ë°˜ RAGì™€ Graph RAGì˜ í•µì‹¬ ì°¨ì´ 3ê°€ì§€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

**ì •ë‹µ:**
1. **ê²€ìƒ‰ ëŒ€ìƒ**: RAGëŠ” í…ìŠ¤íŠ¸ ë¬¸ì„œ, Graph RAGëŠ” ì§€ì‹ ê·¸ë˜í”„
2. **ê´€ê³„ í™œìš©**: RAGëŠ” ê´€ê³„ í™œìš© ë¶ˆê°€, Graph RAGëŠ” ê·¸ë˜í”„ ìˆœíšŒ ë° ì¶”ë¡ 
3. **ì„¤ëª…ë ¥**: RAGëŠ” ì¶œì²˜ë§Œ ì œê³µ, Graph RAGëŠ” ê´€ê³„ ê²½ë¡œê¹Œì§€ ì œê³µ

### ë¬¸ì œ 2: Neuro-Symbolic AI

Neuro-Symbolic AIì˜ ì¥ì  3ê°€ì§€ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”.

**ì •ë‹µ:**
1. ì„¤ëª… ê°€ëŠ¥ì„± (ì™œ ê·¸ëŸ° ê²°ë¡ ì¸ì§€ ì„¤ëª… ê°€ëŠ¥)
2. ì‘ì€ ë°ì´í„° (ì˜¨í†¨ë¡œì§€ ì§€ì‹ìœ¼ë¡œ ë³´ì™„)
3. ì¼ë°˜í™” (ë…¼ë¦¬ ì¶”ë¡ ìœ¼ë¡œ ìƒˆë¡œìš´ ìƒí™© ëŒ€ì‘)

### ë¬¸ì œ 3: Graph RAG ì¿¼ë¦¬

"Teslaì˜ ì°½ì—…ìê°€ ì°½ì—…í•œ ë‹¤ë¥¸ íšŒì‚¬ëŠ”?"ì„ SPARQLë¡œ ì‘ì„±í•˜ì„¸ìš”.

**ì •ë‹µ:**
```sparql
PREFIX schema: <http://schema.org/>

SELECT ?company ?industry
WHERE {
    # Teslaì˜ ì°½ì—…ì ì°¾ê¸°
    ?founder schema:founder :Tesla .
    
    # ê·¸ ì°½ì—…ìê°€ ì°½ì—…í•œ ë‹¤ë¥¸ íšŒì‚¬
    ?founder schema:founder ?company .
    ?company schema:industry ?industry .
    
    # Tesla ì œì™¸
    FILTER(?company != :Tesla)
}
```

---

## ğŸ”— ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
1. "Graph RAG" - Microsoft Research, 2024
2. "Neuro-Symbolic AI" - IBM Research, 2023
3. "Knowledge Graphs for LLMs" - Google AI, 2024

### ë„êµ¬
1. LangChain: https://langchain.com/
2. LlamaIndex: https://www.llamaindex.ai/
3. Neo4j: https://neo4j.com/
4. Weaviate: https://weaviate.io/

### ê¸°ì—…
1. Palantir: https://www.palantir.com/
2. OpenAI: https://openai.com/
3. Anthropic: https://www.anthropic.com/
4. Google DeepMind: https://deepmind.google/

---

**ë‹¤ìŒ:** [Chapter 10: ë¯¸ë˜ íŠ¸ë Œë“œ](./chapter-10.md)

**ì‘ì„±ì:** jeromwolf (ë°ì´í„°ê³µì‘ì†Œ TFT)  
**ì‘ì„±ì¼:** 2025-11-09  
**ë²„ì „:** 3.0 FINAL  
**ë‹¨ì–´ ìˆ˜:** ì•½ 7,500ë‹¨ì–´
